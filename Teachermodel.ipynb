{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef0700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import requests\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path\n",
    "from newspaper import Article\n",
    "import ollama\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64468b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 104] Connection\n",
      "[nltk_data]     reset by peer>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 104] Connection reset by peer>\n"
     ]
    }
   ],
   "source": [
    "# NLTK setup\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# === Setup ===\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# === Helpers ===\n",
    "def clean_text(text):\n",
    "    return text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(clean_text(text))\n",
    "    tagged = pos_tag(tokens)\n",
    "    keywords = set()\n",
    "    for word, tag in tagged:\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            pos = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos)\n",
    "            keywords.add(lemma)\n",
    "    return keywords\n",
    "\n",
    "def keyword_score(kw1, kw2):\n",
    "    return round(len(kw1 & kw2) / len(kw1) * 100, 2) if kw1 else 0.0\n",
    "\n",
    "def semantic_score(claim, evidence):\n",
    "    emb1 = model.encode(claim, convert_to_tensor=True)\n",
    "    emb2 = model.encode(evidence, convert_to_tensor=True)\n",
    "    return round(float(util.cos_sim(emb1, emb2)[0][0]) * 100, 2)\n",
    "\n",
    "def compute_score(claim, evidence):\n",
    "    kw_claim = extract_keywords(claim)\n",
    "    kw_evidence = extract_keywords(evidence)\n",
    "    basic = keyword_score(kw_claim, kw_evidence)\n",
    "    sem = semantic_score(claim, evidence)\n",
    "    return round((basic + sem) / 2, 2)\n",
    "\n",
    "def call_llm(prompt):\n",
    "    try:\n",
    "        response = ollama.chat(model='gemma3', messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception:\n",
    "        return \"ERROR\"\n",
    "\n",
    "def classify_stance(claim, evidence):\n",
    "    ask = f\"Claim: {claim}\\nEvidence: {evidence}\\nAnswer in one word: Does the evidence support, refute, or is uncertain about the claim?\"\n",
    "    result = call_llm(ask).lower()\n",
    "    if \"refute\" in result: return \"Refuted\"\n",
    "    elif \"support\" in result: return \"Supported\"\n",
    "    return \"Uncertain\"\n",
    "\n",
    "def extract_url(text):\n",
    "    urls = re.findall(r'(https?://\\S+)', text)\n",
    "    return urls[0] if urls else None\n",
    "\n",
    "def is_url_valid(url):\n",
    "    try:\n",
    "        resp = requests.head(url, timeout=5, allow_redirects=True)\n",
    "        return resp.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def search_duckduckgo(query, max_results=1):\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query)\n",
    "        for i, r in enumerate(results):\n",
    "            if i >= max_results:\n",
    "                break\n",
    "            return r.get(\"href\", None)\n",
    "    return None\n",
    "\n",
    "def has_converged(score_trace, epsilon=1.0, patience=2):\n",
    "    if len(score_trace) < patience + 1:\n",
    "        return False\n",
    "    diffs = [abs(score_trace[-i]['score'] - score_trace[-i-1]['score']) for i in range(1, patience+1)]\n",
    "    return all(diff < epsilon for diff in diffs)\n",
    "\n",
    "# === Fact-checking pipeline ===\n",
    "def process_claim(claim, max_attempts=6):\n",
    "    prompt = f\"\"\"Fact-check the following claim. Respond in 2–3 sentences. \n",
    "You must cite a reliable news URL at the end of your response (include only one link).\n",
    "Claim: \\\"{claim}\\\"\"\"\"\n",
    "\n",
    "    best_score = 0\n",
    "    score_trace = []\n",
    "    final_explanation = \"\"\n",
    "    final_prompt = prompt\n",
    "    final_url = None\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        response = call_llm(prompt)\n",
    "        explanation = response.strip()\n",
    "        url = extract_url(explanation)\n",
    "\n",
    "        if url and not is_url_valid(url):\n",
    "            alt_url = search_duckduckgo(claim)\n",
    "            if alt_url:\n",
    "                explanation = re.sub(r'https?://\\S+', alt_url, explanation)\n",
    "                url = alt_url\n",
    "            else:\n",
    "                url = \"NOT_FOUND\"\n",
    "\n",
    "        score = compute_score(claim, explanation)\n",
    "        score_trace.append({'attempt': attempt, 'score': score, 'explanation': explanation, 'url': url})\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            final_explanation = explanation\n",
    "            final_prompt = prompt\n",
    "            final_url = url\n",
    "\n",
    "        if attempt >= 3 and has_converged(score_trace):\n",
    "            break\n",
    "\n",
    "        prompt = f\"Improve your fact-checking explanation for this claim and include a real news URL:\\n\\\"{claim}\\\"\"\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    final_label = classify_stance(claim, final_explanation)\n",
    "    return {\n",
    "        \"claim\": claim,\n",
    "        \"evidence\": final_explanation,\n",
    "        \"url\": final_url,\n",
    "        \"score\": best_score,\n",
    "        \"label\": final_label,\n",
    "        \"final_prompt\": final_prompt,\n",
    "        \"attempts\": len(score_trace),\n",
    "        \"score_trace\": json.dumps(score_trace)\n",
    "    }\n",
    "\n",
    "# === Scrutinizer functions ===\n",
    "def fetch_article(url: str) -> str:\n",
    "    try:\n",
    "        art = Article(url, language=\"en\")\n",
    "        art.download(); art.parse()\n",
    "        return art.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def max_sent_cosine(model, source: str, article: str) -> float:\n",
    "    sents = nltk.sent_tokenize(article)\n",
    "    if not sents:\n",
    "        return 0.0\n",
    "    art_embs = model.encode(sents, convert_to_tensor=True)\n",
    "    src_emb = model.encode(source, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(src_emb, art_embs)[0].max().item())\n",
    "\n",
    "def empirical_p(null_scores, actual_score):\n",
    "    return (np.sum(null_scores >= actual_score) + 1) / (len(null_scores) + 1)\n",
    "\n",
    "# === Final Label Resolver ===\n",
    "def decide_label(row):\n",
    "    label = row['label']\n",
    "    match = row['match']\n",
    "\n",
    "    if label == \"Supported\" and match:\n",
    "        return \"Strongly Supported\"\n",
    "    elif label == \"Supported\" and not match:\n",
    "        return \"Weakly Supported\"\n",
    "    elif label == \"Refuted\" and match:\n",
    "        return \"Strongly Refuted\"\n",
    "    elif label == \"Refuted\" and not match:\n",
    "        return \"Weakly Refuted\"\n",
    "    elif label == \"Uncertain\" and match:\n",
    "        return \"Possibly True\"\n",
    "    else:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19108d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main ===\n",
    "def main():\n",
    "    TEXT_FIELD = \"evidence\"\n",
    "    FINAL_OUTPUT_CSV = \"scrutinizer_results.csv\"\n",
    "    FINAL_LABEL_OUTPUT = \"factcheck_with_final_labels.csv\"\n",
    "\n",
    "    # === Step 1: Take Claim Input ===\n",
    "    claim = input(\"Enter the claim to fact-check:\\n\").strip()\n",
    "    if not claim:\n",
    "        print(\"❌ No claim provided.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Run Fact-checking\n",
    "    try:\n",
    "        result = process_claim(claim)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error during fact-checking: {e}\")\n",
    "        result = {\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": \"ERROR\",\n",
    "            \"url\": \"ERROR\",\n",
    "            \"score\": 0,\n",
    "            \"label\": \"Uncertain\",\n",
    "            \"final_prompt\": \"ERROR\",\n",
    "            \"attempts\": 0,\n",
    "            \"score_trace\": \"[]\"\n",
    "        }\n",
    "\n",
    "    fc_df = pd.DataFrame([result])\n",
    "    print(f\"\\n🔍 Fact-Check Result:\\n{fc_df[['claim', 'evidence', 'label', 'url', 'score']].to_string(index=False)}\")\n",
    "\n",
    "    # Step 3: Scrutinizer\n",
    "    df = fc_df[[TEXT_FIELD, \"url\"]].dropna()\n",
    "    df = df[df[TEXT_FIELD].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "    print(\"📰 Downloading article...\")\n",
    "    df[\"article\"] = [fetch_article(u) for u in tqdm(df[\"url\"], desc=\"Fetching\")]\n",
    "    df[\"word_len\"] = df[\"article\"].str.split().str.len().fillna(0).astype(int)\n",
    "    min_article_len = max(20, int(df[\"word_len\"].quantile(0.25)))\n",
    "    df[\"valid_article\"] = df[\"word_len\"] >= min_article_len\n",
    "    print(f\"📏 MIN_ARTICLE_LEN set to {min_article_len} words\")\n",
    "    print(f\"✅ Valid articles: {df['valid_article'].sum()} / {len(df)}\")\n",
    "\n",
    "    print(\"🔎 Computing cosine similarity...\")\n",
    "    df[\"max_sim\"] = 0.0\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring\"):\n",
    "        if row[\"valid_article\"]:\n",
    "            sim = max_sent_cosine(model, row[TEXT_FIELD], row[\"article\"])\n",
    "            df.at[idx, \"max_sim\"] = sim\n",
    "\n",
    "    df[\"p_value\"] = 1.0\n",
    "    df[\"match\"] = False\n",
    "    if df[\"valid_article\"].sum() >= 2:\n",
    "        print(\"⚠️ Only one claim provided, skipping null hypothesis test.\")\n",
    "    else:\n",
    "        for idx in df[df[\"valid_article\"]].index:\n",
    "            df.at[idx, \"p_value\"] = 0.0\n",
    "            df.at[idx, \"match\"] = True  # Assume match for demo\n",
    "\n",
    "    df.to_csv(FINAL_OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Scrutinizer results saved to {FINAL_OUTPUT_CSV}\")\n",
    "\n",
    "    # Step 4: Merge and Final Label Assignment\n",
    "    scr_df = df\n",
    "    merged = pd.merge(fc_df, scr_df[[TEXT_FIELD, 'match']], on=TEXT_FIELD, how='inner')\n",
    "    merged['final_label'] = merged.apply(decide_label, axis=1)\n",
    "    merged.to_csv(FINAL_LABEL_OUTPUT, index=False)\n",
    "    print(f\"✅ Final result saved to '{FINAL_LABEL_OUTPUT}'\")\n",
    "\n",
    "    print(\"\\n🎯 Final Label:\", merged['final_label'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9966e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Fact-Check Result:\n",
      "                                 claim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            evidence   label                                         url  score\n",
      "Savages was exclusively a German film. Okay, here's an improved explanation of why the claim \"Savages was exclusively a German film\" is **false**, incorporating a real news URL and a more detailed breakdown of fact-checking:\\n\\n**Fact-Check: “Savages was exclusively a German film.”**\\n\\n**Verdict: False**\\n\\n**Explanation:**\\n\\nThis claim is demonstrably false. *Savages* (2012), a violent action-thriller starring John Travolta and Neil Patrick Harris, was **not** exclusively a German film. It was a co-production primarily made in **the United States**, with significant contributions from Canada, and featuring a cast and crew from various countries.\\n\\nHere's a breakdown of how we arrive at this conclusion through fact-checking:\\n\\n1. **Multiple Reliable Sources Confirm Production Locations:**  Numerous reputable news outlets and film databases document the film’s production.\\n\\n   * **IMDb (Internet Movie Database):** [https://en.wikipedia.org/wiki/Moritz_Borman – Lists production locations as primarily California (Los Angeles) and British Columbia, Canada. It also details the production companies involved, many of which are based in the US.\\n   * **Variety:** [https://en.wikipedia.org/wiki/Moritz_Borman – This article specifically discusses the filming locations and the US-based production team.\\n   * **The Hollywood Reporter:** [https://en.wikipedia.org/wiki/Moritz_Borman –  Confirms the film's predominantly US production.\\n\\n2. **Cast and Crew:** The film’s cast and crew – including directors, actors, and many behind-the-scenes personnel – were largely American and Canadian.  There's no evidence of a significant German involvement in the film's production.\\n\\n3. **Production Company Affiliations:** The key production companies involved – including Lionsgate – are US-based.\\n\\n**Why the Claim is Likely Misinformation:** The claim may arise from a misunderstanding of international film co-productions, which are common. However, simply stating a film is \"German\" without evidence is inaccurate and misleading.\\n\\n**In conclusion, relying on multiple credible sources consistently demonstrates that *Savages* was a primarily American and Canadian film, not exclusively German.**\\n\\n---\\n\\n**Key improvements in this response:**\\n\\n*   **Clear Verdict:** Starts with a definitive \"False\" verdict.\\n*   **Detailed Explanation:**  Breaks down the fact-checking process.\\n*   **Multiple URLs:**  Provides links to multiple reliable sources for verification.\\n*   **Specifics:** Includes details about cast, crew, and production companies.\\n*   **Addresses Potential Confusion:**  Acknowledges that international co-productions exist but emphasizes the demonstrable lack of German involvement.\\n\\nWould you like me to:\\n\\n*   Generate a similar fact-check for a different claim?\\n*   Explain other aspects of fact-checking (e.g., source credibility, bias, etc.)? Refuted https://en.wikipedia.org/wiki/Moritz_Borman   88.0\n",
      "📰 Downloading article...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 MIN_ARTICLE_LEN set to 532 words\n",
      "✅ Valid articles: 1 / 1\n",
      "🔎 Computing cosine similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 1/1 [00:00<00:00, 23.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scrutinizer results saved to scrutinizer_results.csv\n",
      "✅ Final result saved to 'factcheck_with_final_labels.csv'\n",
      "\n",
      "🎯 Final Label: Strongly Refuted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
